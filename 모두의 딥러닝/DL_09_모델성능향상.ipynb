{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "14장. 모델 성능 향상"
      ],
      "metadata": {
        "id": "WmeI4sA1PcVa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "데이터 확인과 검증셋\n",
        "- 학습이 끝난 모델을 테스트해 보는 것이 테스트셋의 목적이라면, 최적 학습파라미터를 찾기 위해 학습 과정에서 사용하는 것이 검증셋\n",
        "- 검증셋을 설정하면 검증셋에 테스트한 결과를 추적하면서 최적의 모델을 만들 수 있음\n",
        "- 학습셋 : 검증셋 : 테스트셋 = 0.6 : 0.2 : 0.2"
      ],
      "metadata": {
        "id": "VuRsB_AgSPOD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 270,
      "metadata": {
        "id": "u4PxRxio5FfY"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-xSin1OWP1K",
        "outputId": "e58dca90-baf8-4e4c-8d97-e4023c9ebfef"
      },
      "execution_count": 271,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/mulcam_bigdata/data_모두의딥러닝"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HnhCv0ZFWRCZ",
        "outputId": "1bde48dd-4534-478f-fc96-dd3d2cdf3078"
      },
      "execution_count": 272,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/mulcam_bigdata/data_모두의딥러닝\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = './wine.csv'"
      ],
      "metadata": {
        "id": "P6lsyP7bW48G"
      },
      "execution_count": 273,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.read_csv(path, header = None)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "3IMqJRvZXJWL",
        "outputId": "06649592-6f53-499c-b0ca-a061841c1bee"
      },
      "execution_count": 274,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        0     1     2    3      4     5      6        7     8     9     10  \\\n",
              "0      7.4  0.70  0.00  1.9  0.076  11.0   34.0  0.99780  3.51  0.56   9.4   \n",
              "1      7.8  0.88  0.00  2.6  0.098  25.0   67.0  0.99680  3.20  0.68   9.8   \n",
              "2      7.8  0.76  0.04  2.3  0.092  15.0   54.0  0.99700  3.26  0.65   9.8   \n",
              "3     11.2  0.28  0.56  1.9  0.075  17.0   60.0  0.99800  3.16  0.58   9.8   \n",
              "4      7.4  0.70  0.00  1.9  0.076  11.0   34.0  0.99780  3.51  0.56   9.4   \n",
              "...    ...   ...   ...  ...    ...   ...    ...      ...   ...   ...   ...   \n",
              "6492   6.2  0.21  0.29  1.6  0.039  24.0   92.0  0.99114  3.27  0.50  11.2   \n",
              "6493   6.6  0.32  0.36  8.0  0.047  57.0  168.0  0.99490  3.15  0.46   9.6   \n",
              "6494   6.5  0.24  0.19  1.2  0.041  30.0  111.0  0.99254  2.99  0.46   9.4   \n",
              "6495   5.5  0.29  0.30  1.1  0.022  20.0  110.0  0.98869  3.34  0.38  12.8   \n",
              "6496   6.0  0.21  0.38  0.8  0.020  22.0   98.0  0.98941  3.26  0.32  11.8   \n",
              "\n",
              "      11  12  \n",
              "0      5   1  \n",
              "1      5   1  \n",
              "2      5   1  \n",
              "3      6   1  \n",
              "4      5   1  \n",
              "...   ..  ..  \n",
              "6492   6   0  \n",
              "6493   5   0  \n",
              "6494   6   0  \n",
              "6495   7   0  \n",
              "6496   6   0  \n",
              "\n",
              "[6497 rows x 13 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fef07616-2d73-492e-afe1-2916b01046c5\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7.4</td>\n",
              "      <td>0.70</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.9</td>\n",
              "      <td>0.076</td>\n",
              "      <td>11.0</td>\n",
              "      <td>34.0</td>\n",
              "      <td>0.99780</td>\n",
              "      <td>3.51</td>\n",
              "      <td>0.56</td>\n",
              "      <td>9.4</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>7.8</td>\n",
              "      <td>0.88</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2.6</td>\n",
              "      <td>0.098</td>\n",
              "      <td>25.0</td>\n",
              "      <td>67.0</td>\n",
              "      <td>0.99680</td>\n",
              "      <td>3.20</td>\n",
              "      <td>0.68</td>\n",
              "      <td>9.8</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7.8</td>\n",
              "      <td>0.76</td>\n",
              "      <td>0.04</td>\n",
              "      <td>2.3</td>\n",
              "      <td>0.092</td>\n",
              "      <td>15.0</td>\n",
              "      <td>54.0</td>\n",
              "      <td>0.99700</td>\n",
              "      <td>3.26</td>\n",
              "      <td>0.65</td>\n",
              "      <td>9.8</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11.2</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.56</td>\n",
              "      <td>1.9</td>\n",
              "      <td>0.075</td>\n",
              "      <td>17.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>0.99800</td>\n",
              "      <td>3.16</td>\n",
              "      <td>0.58</td>\n",
              "      <td>9.8</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7.4</td>\n",
              "      <td>0.70</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.9</td>\n",
              "      <td>0.076</td>\n",
              "      <td>11.0</td>\n",
              "      <td>34.0</td>\n",
              "      <td>0.99780</td>\n",
              "      <td>3.51</td>\n",
              "      <td>0.56</td>\n",
              "      <td>9.4</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6492</th>\n",
              "      <td>6.2</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.29</td>\n",
              "      <td>1.6</td>\n",
              "      <td>0.039</td>\n",
              "      <td>24.0</td>\n",
              "      <td>92.0</td>\n",
              "      <td>0.99114</td>\n",
              "      <td>3.27</td>\n",
              "      <td>0.50</td>\n",
              "      <td>11.2</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6493</th>\n",
              "      <td>6.6</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.36</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.047</td>\n",
              "      <td>57.0</td>\n",
              "      <td>168.0</td>\n",
              "      <td>0.99490</td>\n",
              "      <td>3.15</td>\n",
              "      <td>0.46</td>\n",
              "      <td>9.6</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6494</th>\n",
              "      <td>6.5</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.19</td>\n",
              "      <td>1.2</td>\n",
              "      <td>0.041</td>\n",
              "      <td>30.0</td>\n",
              "      <td>111.0</td>\n",
              "      <td>0.99254</td>\n",
              "      <td>2.99</td>\n",
              "      <td>0.46</td>\n",
              "      <td>9.4</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6495</th>\n",
              "      <td>5.5</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.30</td>\n",
              "      <td>1.1</td>\n",
              "      <td>0.022</td>\n",
              "      <td>20.0</td>\n",
              "      <td>110.0</td>\n",
              "      <td>0.98869</td>\n",
              "      <td>3.34</td>\n",
              "      <td>0.38</td>\n",
              "      <td>12.8</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6496</th>\n",
              "      <td>6.0</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0.020</td>\n",
              "      <td>22.0</td>\n",
              "      <td>98.0</td>\n",
              "      <td>0.98941</td>\n",
              "      <td>3.26</td>\n",
              "      <td>0.32</td>\n",
              "      <td>11.8</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6497 rows × 13 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fef07616-2d73-492e-afe1-2916b01046c5')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-fef07616-2d73-492e-afe1-2916b01046c5 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-fef07616-2d73-492e-afe1-2916b01046c5');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-90763b53-2cb1-4fda-9914-0ebf0d17a9c2\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-90763b53-2cb1-4fda-9914-0ebf0d17a9c2')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-90763b53-2cb1-4fda-9914-0ebf0d17a9c2 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"pd\",\n  \"rows\": 6497,\n  \"fields\": [\n    {\n      \"column\": 0,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.2964337577998153,\n        \"min\": 3.8,\n        \"max\": 15.9,\n        \"num_unique_values\": 106,\n        \"samples\": [\n          7.15,\n          8.1,\n          7.3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 1,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.16463647408467877,\n        \"min\": 0.08,\n        \"max\": 1.58,\n        \"num_unique_values\": 187,\n        \"samples\": [\n          0.405,\n          0.21,\n          0.695\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 2,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.14531786489759155,\n        \"min\": 0.0,\n        \"max\": 1.66,\n        \"num_unique_values\": 89,\n        \"samples\": [\n          0.1,\n          0.6,\n          0.37\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 3,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4.757803743147418,\n        \"min\": 0.6,\n        \"max\": 65.8,\n        \"num_unique_values\": 316,\n        \"samples\": [\n          18.95,\n          3.2,\n          9.3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 4,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.03503360137245907,\n        \"min\": 0.009,\n        \"max\": 0.611,\n        \"num_unique_values\": 214,\n        \"samples\": [\n          0.089,\n          0.217,\n          0.1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 5,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 17.7493997720025,\n        \"min\": 1.0,\n        \"max\": 289.0,\n        \"num_unique_values\": 135,\n        \"samples\": [\n          77.5,\n          65.0,\n          128.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 6,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 56.521854522630285,\n        \"min\": 6.0,\n        \"max\": 440.0,\n        \"num_unique_values\": 276,\n        \"samples\": [\n          14.0,\n          149.0,\n          227.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 7,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0029986730037190393,\n        \"min\": 0.98711,\n        \"max\": 1.03898,\n        \"num_unique_values\": 998,\n        \"samples\": [\n          0.9918,\n          0.99412,\n          0.99484\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 8,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.16078720210398764,\n        \"min\": 2.72,\n        \"max\": 4.01,\n        \"num_unique_values\": 108,\n        \"samples\": [\n          3.74,\n          3.17,\n          3.3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 9,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.14880587361449027,\n        \"min\": 0.22,\n        \"max\": 2.0,\n        \"num_unique_values\": 111,\n        \"samples\": [\n          1.11,\n          1.56,\n          0.46\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 10,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.192711748870993,\n        \"min\": 8.0,\n        \"max\": 14.9,\n        \"num_unique_values\": 111,\n        \"samples\": [\n          10.9333333333333,\n          9.7,\n          10.5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 11,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 3,\n        \"max\": 9,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          5,\n          6,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 12,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 274
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(path, header = None)"
      ],
      "metadata": {
        "id": "VKIwc5nyXMyD"
      },
      "execution_count": 275,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upzl-wSrXQlv",
        "outputId": "b5ad32f6-f02d-4738-bead-e84a0cdd6d00"
      },
      "execution_count": 276,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6497, 13)"
            ]
          },
          "metadata": {},
          "execution_count": 276
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 독립변수(x) : wine 속성\n",
        "# 종속변수(y) : 와인분류라벨(red wine / white wine)\n",
        "\n",
        "df[:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "LDmnxk14XSDx",
        "outputId": "8b310a27-9fb5-4943-ea38-d9dcc38cab51"
      },
      "execution_count": 277,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    0     1     2    3      4     5     6       7     8     9    10  11  12\n",
              "0  7.4  0.70  0.00  1.9  0.076  11.0  34.0  0.9978  3.51  0.56  9.4   5   1\n",
              "1  7.8  0.88  0.00  2.6  0.098  25.0  67.0  0.9968  3.20  0.68  9.8   5   1\n",
              "2  7.8  0.76  0.04  2.3  0.092  15.0  54.0  0.9970  3.26  0.65  9.8   5   1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9048ef12-4179-44be-b6da-4e279750b11e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7.4</td>\n",
              "      <td>0.70</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.9</td>\n",
              "      <td>0.076</td>\n",
              "      <td>11.0</td>\n",
              "      <td>34.0</td>\n",
              "      <td>0.9978</td>\n",
              "      <td>3.51</td>\n",
              "      <td>0.56</td>\n",
              "      <td>9.4</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>7.8</td>\n",
              "      <td>0.88</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2.6</td>\n",
              "      <td>0.098</td>\n",
              "      <td>25.0</td>\n",
              "      <td>67.0</td>\n",
              "      <td>0.9968</td>\n",
              "      <td>3.20</td>\n",
              "      <td>0.68</td>\n",
              "      <td>9.8</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7.8</td>\n",
              "      <td>0.76</td>\n",
              "      <td>0.04</td>\n",
              "      <td>2.3</td>\n",
              "      <td>0.092</td>\n",
              "      <td>15.0</td>\n",
              "      <td>54.0</td>\n",
              "      <td>0.9970</td>\n",
              "      <td>3.26</td>\n",
              "      <td>0.65</td>\n",
              "      <td>9.8</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9048ef12-4179-44be-b6da-4e279750b11e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9048ef12-4179-44be-b6da-4e279750b11e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9048ef12-4179-44be-b6da-4e279750b11e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-a29f0e88-2138-477b-9228-fa337f263209\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a29f0e88-2138-477b-9228-fa337f263209')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-a29f0e88-2138-477b-9228-fa337f263209 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df[:3]\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": 0,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.23094010767585002,\n        \"min\": 7.4,\n        \"max\": 7.8,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          7.8,\n          7.4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 1,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.09165151389911683,\n        \"min\": 0.7,\n        \"max\": 0.88,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.7,\n          0.88\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 2,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.023094010767585032,\n        \"min\": 0.0,\n        \"max\": 0.04,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.04,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 3,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.3511884584284247,\n        \"min\": 1.9,\n        \"max\": 2.6,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1.9,\n          2.6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 4,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.011372481406154655,\n        \"min\": 0.076,\n        \"max\": 0.098,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.076,\n          0.098\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 5,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7.211102550927978,\n        \"min\": 11.0,\n        \"max\": 25.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          11.0,\n          25.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 6,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 16.623276853055575,\n        \"min\": 34.0,\n        \"max\": 67.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          34.0,\n          67.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 7,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0005291502622129227,\n        \"min\": 0.9968,\n        \"max\": 0.9978,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.9978,\n          0.9968\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 8,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.1644181660685135,\n        \"min\": 3.2,\n        \"max\": 3.51,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          3.51,\n          3.2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 9,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.06244997998398397,\n        \"min\": 0.56,\n        \"max\": 0.68,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.56,\n          0.68\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 10,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.23094010767585052,\n        \"min\": 9.4,\n        \"max\": 9.8,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          9.8,\n          9.4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 11,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 5,\n        \"max\": 5,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 12,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 277
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = df.iloc[:, :-1]\n",
        "y = df.iloc[:, -1]"
      ],
      "metadata": {
        "id": "kFa7MpR0XS3s"
      },
      "execution_count": 278,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_test set\n",
        "x_train, x_test, y_train, y_test = \\\n",
        "train_test_split(x,y, test_size = 0.2, shuffle = True)"
      ],
      "metadata": {
        "id": "J6HdVVRrXaQY"
      },
      "execution_count": 279,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 구조 설정\n",
        "model = Sequential()\n",
        "model.add(Dense(30, input_dim = 12, activation = 'relu'))\n",
        "model.add(Dense(12,activation = 'relu'))\n",
        "model.add(Dense(8, activation = 'relu'))\n",
        "model.add(Dense(1, activation = 'sigmoid'))\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1tXU-nQXoVB",
        "outputId": "c3661ceb-1c29-42c0-a6e9-e4e4f4ebdd58"
      },
      "execution_count": 280,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_36 (Dense)            (None, 30)                390       \n",
            "                                                                 \n",
            " dense_37 (Dense)            (None, 12)                372       \n",
            "                                                                 \n",
            " dense_38 (Dense)            (None, 8)                 104       \n",
            "                                                                 \n",
            " dense_39 (Dense)            (None, 1)                 9         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 875 (3.42 KB)\n",
            "Trainable params: 875 (3.42 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 컴파일\n",
        "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics=['accuracy'])\n",
        "# 'binary_crossentropy'는 이진 분류 문제에 사용되는 손실 함수\n",
        "# 'adam'은 최적화 알고리즘\n",
        "# accuracy는 모델의 정확도를 평가하는 메트릭"
      ],
      "metadata": {
        "id": "o9fAq6_5l6Wc"
      },
      "execution_count": 281,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "YQRVjP2s0ojx",
        "outputId": "7e91bf09-3ea0-4780-e51d-d811856ea301"
      },
      "execution_count": 282,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/mulcam_bigdata/data_모두의딥러닝'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 282
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "검증셋 적용"
      ],
      "metadata": {
        "id": "r9hBtQLFdsNM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 실행(fit >> train data)\n",
        "\n",
        "history = model.fit(x_train, y_train, epochs = 50, batch_size = 500,\n",
        "                    validation_split = 0.25)\n",
        "                    # train data의 25%를 validation data로 활용"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJHGfRj_UGeM",
        "outputId": "50e221c1-01e2-4ba6-8dd7-e702af6f687d"
      },
      "execution_count": 283,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "8/8 [==============================] - 2s 39ms/step - loss: 1.3013 - accuracy: 0.7452 - val_loss: 0.6687 - val_accuracy: 0.7785\n",
            "Epoch 2/50\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5585 - accuracy: 0.7398 - val_loss: 0.5343 - val_accuracy: 0.7477\n",
            "Epoch 3/50\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.4208 - accuracy: 0.7691 - val_loss: 0.3433 - val_accuracy: 0.8062\n",
            "Epoch 4/50\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.3729 - accuracy: 0.8042 - val_loss: 0.2879 - val_accuracy: 0.8869\n",
            "Epoch 5/50\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.3173 - accuracy: 0.9009 - val_loss: 0.2878 - val_accuracy: 0.9285\n",
            "Epoch 6/50\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.2931 - accuracy: 0.9079 - val_loss: 0.2575 - val_accuracy: 0.9162\n",
            "Epoch 7/50\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2721 - accuracy: 0.9135 - val_loss: 0.2516 - val_accuracy: 0.9392\n",
            "Epoch 8/50\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.2599 - accuracy: 0.9269 - val_loss: 0.2380 - val_accuracy: 0.9392\n",
            "Epoch 9/50\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.2498 - accuracy: 0.9238 - val_loss: 0.2295 - val_accuracy: 0.9377\n",
            "Epoch 10/50\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.2386 - accuracy: 0.9297 - val_loss: 0.2231 - val_accuracy: 0.9392\n",
            "Epoch 11/50\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.2298 - accuracy: 0.9312 - val_loss: 0.2115 - val_accuracy: 0.9400\n",
            "Epoch 12/50\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.2216 - accuracy: 0.9307 - val_loss: 0.2061 - val_accuracy: 0.9400\n",
            "Epoch 13/50\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.2127 - accuracy: 0.9312 - val_loss: 0.2000 - val_accuracy: 0.9415\n",
            "Epoch 14/50\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.2058 - accuracy: 0.9328 - val_loss: 0.1964 - val_accuracy: 0.9431\n",
            "Epoch 15/50\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1997 - accuracy: 0.9335 - val_loss: 0.1937 - val_accuracy: 0.9408\n",
            "Epoch 16/50\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.1959 - accuracy: 0.9341 - val_loss: 0.1930 - val_accuracy: 0.9415\n",
            "Epoch 17/50\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.1925 - accuracy: 0.9346 - val_loss: 0.1888 - val_accuracy: 0.9408\n",
            "Epoch 18/50\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.1890 - accuracy: 0.9353 - val_loss: 0.1855 - val_accuracy: 0.9400\n",
            "Epoch 19/50\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.1832 - accuracy: 0.9361 - val_loss: 0.1821 - val_accuracy: 0.9423\n",
            "Epoch 20/50\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1797 - accuracy: 0.9384 - val_loss: 0.1772 - val_accuracy: 0.9446\n",
            "Epoch 21/50\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.1768 - accuracy: 0.9364 - val_loss: 0.1768 - val_accuracy: 0.9431\n",
            "Epoch 22/50\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.1747 - accuracy: 0.9382 - val_loss: 0.1706 - val_accuracy: 0.9438\n",
            "Epoch 23/50\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.1738 - accuracy: 0.9364 - val_loss: 0.1749 - val_accuracy: 0.9462\n",
            "Epoch 24/50\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1684 - accuracy: 0.9397 - val_loss: 0.1699 - val_accuracy: 0.9431\n",
            "Epoch 25/50\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.1662 - accuracy: 0.9374 - val_loss: 0.1677 - val_accuracy: 0.9462\n",
            "Epoch 26/50\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1648 - accuracy: 0.9400 - val_loss: 0.1650 - val_accuracy: 0.9431\n",
            "Epoch 27/50\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1635 - accuracy: 0.9394 - val_loss: 0.1651 - val_accuracy: 0.9469\n",
            "Epoch 28/50\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.1602 - accuracy: 0.9405 - val_loss: 0.1603 - val_accuracy: 0.9469\n",
            "Epoch 29/50\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1590 - accuracy: 0.9410 - val_loss: 0.1629 - val_accuracy: 0.9485\n",
            "Epoch 30/50\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1563 - accuracy: 0.9420 - val_loss: 0.1577 - val_accuracy: 0.9462\n",
            "Epoch 31/50\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1547 - accuracy: 0.9425 - val_loss: 0.1572 - val_accuracy: 0.9462\n",
            "Epoch 32/50\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1534 - accuracy: 0.9448 - val_loss: 0.1555 - val_accuracy: 0.9469\n",
            "Epoch 33/50\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.1516 - accuracy: 0.9423 - val_loss: 0.1583 - val_accuracy: 0.9492\n",
            "Epoch 34/50\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1498 - accuracy: 0.9471 - val_loss: 0.1521 - val_accuracy: 0.9500\n",
            "Epoch 35/50\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.1499 - accuracy: 0.9435 - val_loss: 0.1531 - val_accuracy: 0.9492\n",
            "Epoch 36/50\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1464 - accuracy: 0.9464 - val_loss: 0.1496 - val_accuracy: 0.9515\n",
            "Epoch 37/50\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1454 - accuracy: 0.9484 - val_loss: 0.1520 - val_accuracy: 0.9492\n",
            "Epoch 38/50\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.1437 - accuracy: 0.9479 - val_loss: 0.1457 - val_accuracy: 0.9515\n",
            "Epoch 39/50\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.1418 - accuracy: 0.9479 - val_loss: 0.1460 - val_accuracy: 0.9523\n",
            "Epoch 40/50\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.1401 - accuracy: 0.9492 - val_loss: 0.1457 - val_accuracy: 0.9515\n",
            "Epoch 41/50\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1388 - accuracy: 0.9492 - val_loss: 0.1413 - val_accuracy: 0.9531\n",
            "Epoch 42/50\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.1371 - accuracy: 0.9482 - val_loss: 0.1426 - val_accuracy: 0.9531\n",
            "Epoch 43/50\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.1362 - accuracy: 0.9500 - val_loss: 0.1389 - val_accuracy: 0.9538\n",
            "Epoch 44/50\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.1349 - accuracy: 0.9479 - val_loss: 0.1437 - val_accuracy: 0.9523\n",
            "Epoch 45/50\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.1354 - accuracy: 0.9479 - val_loss: 0.1383 - val_accuracy: 0.9554\n",
            "Epoch 46/50\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.1322 - accuracy: 0.9502 - val_loss: 0.1341 - val_accuracy: 0.9546\n",
            "Epoch 47/50\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1302 - accuracy: 0.9497 - val_loss: 0.1335 - val_accuracy: 0.9562\n",
            "Epoch 48/50\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1285 - accuracy: 0.9520 - val_loss: 0.1313 - val_accuracy: 0.9546\n",
            "Epoch 49/50\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.1279 - accuracy: 0.9515 - val_loss: 0.1360 - val_accuracy: 0.9531\n",
            "Epoch 50/50\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1295 - accuracy: 0.9507 - val_loss: 0.1325 - val_accuracy: 0.9562\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 테스트 결과 출력\n",
        "score = model.evaluate(x_test, y_test)\n",
        "print('Test Dataset ACC: ', round(score[1],2))\n",
        "# Test Dataset ACC: 0.98"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_rQPPECULcr",
        "outputId": "0932147d-1727-43a6-d03f-b39a3f9ca126"
      },
      "execution_count": 284,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "41/41 [==============================] - 0s 2ms/step - loss: 0.1161 - accuracy: 0.9600\n",
            "Test Dataset ACC:  0.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델 업데이트"
      ],
      "metadata": {
        "id": "bjTLDBNaVslG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/mulcam_bigdata/09_DL/MODEL"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTGZlOfzjpRi",
        "outputId": "1c8b393e-5654-4b07-c49c-0d93afb5443c"
      },
      "execution_count": 285,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/mulcam_bigdata/09_DL/MODEL\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 저장 설정\n",
        "modelpath = './{epoch:02d}-{val_accuracy:.04f}.hdf5'\n",
        "# 플레이스홀더({epoch:02d}-{val_accuracy:.04f}.hdf5)는 모델을 저장할 때 에포크 번호와 검증 정확도로 대체"
      ],
      "metadata": {
        "id": "YjKLG6BOV1RL"
      },
      "execution_count": 286,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 성능을 식별하는데 도움\n",
        "checkpointer = ModelCheckpoint(filepath=modelpath, verbose=1, save_best_only=True)\n",
        "# checkpointer = ModelCheckpoint(filepath=modelpath, verbose=1, save_best_only=True)\n",
        "# verbose=1 현황 모니터링 가능\n",
        "# save_best_only=True"
      ],
      "metadata": {
        "id": "umBmQpCFVXPx"
      },
      "execution_count": 287,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 실행(fit >> train data)\n",
        "\n",
        "history = model.fit(x_train, y_train, epochs = 50, batch_size = 500,\n",
        "                    validation_split = 0.25, verbose=0, callbacks = [checkpointer])\n",
        "\n",
        "# callbacks = [] : 훈련 과정 중 특정 시점에 호출되는 콜백함수 목록\n",
        "# >> checkpointer : 모델의 검증 정확도가 개선될 때마다 모델을 저장"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "skfoJD163OV6",
        "outputId": "5f5aabd4-caff-4f92-8c5d-69def7186310"
      },
      "execution_count": 288,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1: val_loss improved from inf to 0.12889, saving model to ./01-0.9554.hdf5\n",
            "\n",
            "Epoch 2: val_loss improved from 0.12889 to 0.12711, saving model to ./02-0.9569.hdf5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 3: val_loss improved from 0.12711 to 0.12430, saving model to ./03-0.9585.hdf5\n",
            "\n",
            "Epoch 4: val_loss improved from 0.12430 to 0.11982, saving model to ./04-0.9638.hdf5\n",
            "\n",
            "Epoch 5: val_loss improved from 0.11982 to 0.11636, saving model to ./05-0.9654.hdf5\n",
            "\n",
            "Epoch 6: val_loss improved from 0.11636 to 0.11218, saving model to ./06-0.9608.hdf5\n",
            "\n",
            "Epoch 7: val_loss improved from 0.11218 to 0.11144, saving model to ./07-0.9662.hdf5\n",
            "\n",
            "Epoch 8: val_loss improved from 0.11144 to 0.11143, saving model to ./08-0.9669.hdf5\n",
            "\n",
            "Epoch 9: val_loss improved from 0.11143 to 0.09839, saving model to ./09-0.9700.hdf5\n",
            "\n",
            "Epoch 10: val_loss did not improve from 0.09839\n",
            "\n",
            "Epoch 11: val_loss did not improve from 0.09839\n",
            "\n",
            "Epoch 12: val_loss improved from 0.09839 to 0.09075, saving model to ./12-0.9715.hdf5\n",
            "\n",
            "Epoch 13: val_loss improved from 0.09075 to 0.08888, saving model to ./13-0.9715.hdf5\n",
            "\n",
            "Epoch 14: val_loss did not improve from 0.08888\n",
            "\n",
            "Epoch 15: val_loss improved from 0.08888 to 0.08819, saving model to ./15-0.9715.hdf5\n",
            "\n",
            "Epoch 16: val_loss improved from 0.08819 to 0.08534, saving model to ./16-0.9731.hdf5\n",
            "\n",
            "Epoch 17: val_loss improved from 0.08534 to 0.08412, saving model to ./17-0.9754.hdf5\n",
            "\n",
            "Epoch 18: val_loss improved from 0.08412 to 0.08376, saving model to ./18-0.9746.hdf5\n",
            "\n",
            "Epoch 19: val_loss improved from 0.08376 to 0.08247, saving model to ./19-0.9754.hdf5\n",
            "\n",
            "Epoch 20: val_loss did not improve from 0.08247\n",
            "\n",
            "Epoch 21: val_loss did not improve from 0.08247\n",
            "\n",
            "Epoch 22: val_loss improved from 0.08247 to 0.08003, saving model to ./22-0.9754.hdf5\n",
            "\n",
            "Epoch 23: val_loss did not improve from 0.08003\n",
            "\n",
            "Epoch 24: val_loss did not improve from 0.08003\n",
            "\n",
            "Epoch 25: val_loss did not improve from 0.08003\n",
            "\n",
            "Epoch 26: val_loss improved from 0.08003 to 0.07863, saving model to ./26-0.9746.hdf5\n",
            "\n",
            "Epoch 27: val_loss did not improve from 0.07863\n",
            "\n",
            "Epoch 28: val_loss did not improve from 0.07863\n",
            "\n",
            "Epoch 29: val_loss improved from 0.07863 to 0.07598, saving model to ./29-0.9762.hdf5\n",
            "\n",
            "Epoch 30: val_loss improved from 0.07598 to 0.07534, saving model to ./30-0.9762.hdf5\n",
            "\n",
            "Epoch 31: val_loss did not improve from 0.07534\n",
            "\n",
            "Epoch 32: val_loss did not improve from 0.07534\n",
            "\n",
            "Epoch 33: val_loss did not improve from 0.07534\n",
            "\n",
            "Epoch 34: val_loss did not improve from 0.07534\n",
            "\n",
            "Epoch 35: val_loss did not improve from 0.07534\n",
            "\n",
            "Epoch 36: val_loss did not improve from 0.07534\n",
            "\n",
            "Epoch 37: val_loss improved from 0.07534 to 0.07375, saving model to ./37-0.9769.hdf5\n",
            "\n",
            "Epoch 38: val_loss improved from 0.07375 to 0.07315, saving model to ./38-0.9792.hdf5\n",
            "\n",
            "Epoch 39: val_loss improved from 0.07315 to 0.07181, saving model to ./39-0.9800.hdf5\n",
            "\n",
            "Epoch 40: val_loss improved from 0.07181 to 0.07105, saving model to ./40-0.9800.hdf5\n",
            "\n",
            "Epoch 41: val_loss improved from 0.07105 to 0.07094, saving model to ./41-0.9808.hdf5\n",
            "\n",
            "Epoch 42: val_loss improved from 0.07094 to 0.07043, saving model to ./42-0.9808.hdf5\n",
            "\n",
            "Epoch 43: val_loss improved from 0.07043 to 0.07027, saving model to ./43-0.9792.hdf5\n",
            "\n",
            "Epoch 44: val_loss improved from 0.07027 to 0.06936, saving model to ./44-0.9800.hdf5\n",
            "\n",
            "Epoch 45: val_loss improved from 0.06936 to 0.06902, saving model to ./45-0.9792.hdf5\n",
            "\n",
            "Epoch 46: val_loss improved from 0.06902 to 0.06888, saving model to ./46-0.9785.hdf5\n",
            "\n",
            "Epoch 47: val_loss did not improve from 0.06888\n",
            "\n",
            "Epoch 48: val_loss did not improve from 0.06888\n",
            "\n",
            "Epoch 49: val_loss improved from 0.06888 to 0.06831, saving model to ./49-0.9800.hdf5\n",
            "\n",
            "Epoch 50: val_loss improved from 0.06831 to 0.06761, saving model to ./50-0.9808.hdf5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 테스트 결과 출력\n",
        "score = model.evaluate(x_test, y_test)\n",
        "print('Test Dataset ACC: ', round(score[1],2))\n",
        "# Test Dataset ACC: 0.98"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYAwBVTe1TWI",
        "outputId": "ea0f6b56-4f5f-444a-fea3-88dba7f0a89e"
      },
      "execution_count": 289,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "41/41 [==============================] - 0s 2ms/step - loss: 0.0671 - accuracy: 0.9808\n",
            "Test Dataset ACC:  0.98\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "과적합 확인하기"
      ],
      "metadata": {
        "id": "4KMc81AnZmhx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(x_train, y_train, epochs = 50, batch_size = 500,\n",
        "                    validation_split = 0.25, verbose=0, callbacks = [checkpointer])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xLc0R5zeZhU",
        "outputId": "9f8f3b95-7c5e-48e3-a653-2de4fd2905fb"
      },
      "execution_count": 290,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1: val_loss improved from 0.06761 to 0.06758, saving model to ./01-0.9792.hdf5\n",
            "\n",
            "Epoch 2: val_loss improved from 0.06758 to 0.06643, saving model to ./02-0.9792.hdf5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 3: val_loss did not improve from 0.06643\n",
            "\n",
            "Epoch 4: val_loss did not improve from 0.06643\n",
            "\n",
            "Epoch 5: val_loss did not improve from 0.06643\n",
            "\n",
            "Epoch 6: val_loss improved from 0.06643 to 0.06558, saving model to ./06-0.9808.hdf5\n",
            "\n",
            "Epoch 7: val_loss did not improve from 0.06558\n",
            "\n",
            "Epoch 8: val_loss did not improve from 0.06558\n",
            "\n",
            "Epoch 9: val_loss improved from 0.06558 to 0.06445, saving model to ./09-0.9808.hdf5\n",
            "\n",
            "Epoch 10: val_loss improved from 0.06445 to 0.06379, saving model to ./10-0.9808.hdf5\n",
            "\n",
            "Epoch 11: val_loss did not improve from 0.06379\n",
            "\n",
            "Epoch 12: val_loss did not improve from 0.06379\n",
            "\n",
            "Epoch 13: val_loss did not improve from 0.06379\n",
            "\n",
            "Epoch 14: val_loss improved from 0.06379 to 0.06307, saving model to ./14-0.9800.hdf5\n",
            "\n",
            "Epoch 15: val_loss improved from 0.06307 to 0.06296, saving model to ./15-0.9815.hdf5\n",
            "\n",
            "Epoch 16: val_loss did not improve from 0.06296\n",
            "\n",
            "Epoch 17: val_loss improved from 0.06296 to 0.06275, saving model to ./17-0.9808.hdf5\n",
            "\n",
            "Epoch 18: val_loss did not improve from 0.06275\n",
            "\n",
            "Epoch 19: val_loss improved from 0.06275 to 0.06156, saving model to ./19-0.9815.hdf5\n",
            "\n",
            "Epoch 20: val_loss did not improve from 0.06156\n",
            "\n",
            "Epoch 21: val_loss did not improve from 0.06156\n",
            "\n",
            "Epoch 22: val_loss improved from 0.06156 to 0.06105, saving model to ./22-0.9823.hdf5\n",
            "\n",
            "Epoch 23: val_loss did not improve from 0.06105\n",
            "\n",
            "Epoch 24: val_loss did not improve from 0.06105\n",
            "\n",
            "Epoch 25: val_loss did not improve from 0.06105\n",
            "\n",
            "Epoch 26: val_loss did not improve from 0.06105\n",
            "\n",
            "Epoch 27: val_loss did not improve from 0.06105\n",
            "\n",
            "Epoch 28: val_loss did not improve from 0.06105\n",
            "\n",
            "Epoch 29: val_loss improved from 0.06105 to 0.06081, saving model to ./29-0.9823.hdf5\n",
            "\n",
            "Epoch 30: val_loss improved from 0.06081 to 0.06038, saving model to ./30-0.9808.hdf5\n",
            "\n",
            "Epoch 31: val_loss did not improve from 0.06038\n",
            "\n",
            "Epoch 32: val_loss did not improve from 0.06038\n",
            "\n",
            "Epoch 33: val_loss did not improve from 0.06038\n",
            "\n",
            "Epoch 34: val_loss improved from 0.06038 to 0.05897, saving model to ./34-0.9823.hdf5\n",
            "\n",
            "Epoch 35: val_loss did not improve from 0.05897\n",
            "\n",
            "Epoch 36: val_loss did not improve from 0.05897\n",
            "\n",
            "Epoch 37: val_loss did not improve from 0.05897\n",
            "\n",
            "Epoch 38: val_loss improved from 0.05897 to 0.05829, saving model to ./38-0.9823.hdf5\n",
            "\n",
            "Epoch 39: val_loss did not improve from 0.05829\n",
            "\n",
            "Epoch 40: val_loss did not improve from 0.05829\n",
            "\n",
            "Epoch 41: val_loss did not improve from 0.05829\n",
            "\n",
            "Epoch 42: val_loss improved from 0.05829 to 0.05783, saving model to ./42-0.9831.hdf5\n",
            "\n",
            "Epoch 43: val_loss improved from 0.05783 to 0.05719, saving model to ./43-0.9823.hdf5\n",
            "\n",
            "Epoch 44: val_loss did not improve from 0.05719\n",
            "\n",
            "Epoch 45: val_loss did not improve from 0.05719\n",
            "\n",
            "Epoch 46: val_loss did not improve from 0.05719\n",
            "\n",
            "Epoch 47: val_loss did not improve from 0.05719\n",
            "\n",
            "Epoch 48: val_loss did not improve from 0.05719\n",
            "\n",
            "Epoch 49: val_loss did not improve from 0.05719\n",
            "\n",
            "Epoch 50: val_loss improved from 0.05719 to 0.05665, saving model to ./50-0.9831.hdf5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history.history\n",
        "# loss, accuracy, val_loss, val_accuracy 는 history.history 에 포함되어 있음"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_uAzQ__DKa8",
        "outputId": "71b423b9-4193-46d0-fc60-53ea7cec1579"
      },
      "execution_count": 291,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'loss': [0.06834335625171661,\n",
              "  0.06652641296386719,\n",
              "  0.06523549556732178,\n",
              "  0.06581481546163559,\n",
              "  0.06591975688934326,\n",
              "  0.0656251534819603,\n",
              "  0.06499543786048889,\n",
              "  0.06440373510122299,\n",
              "  0.06534858793020248,\n",
              "  0.06329808384180069,\n",
              "  0.0644339993596077,\n",
              "  0.06503696739673615,\n",
              "  0.06838509440422058,\n",
              "  0.06293390691280365,\n",
              "  0.06339912861585617,\n",
              "  0.06319455802440643,\n",
              "  0.06398016959428787,\n",
              "  0.06263968348503113,\n",
              "  0.06339363008737564,\n",
              "  0.06218142434954643,\n",
              "  0.06554005295038223,\n",
              "  0.06611580401659012,\n",
              "  0.06463051587343216,\n",
              "  0.06130717694759369,\n",
              "  0.06065637618303299,\n",
              "  0.05970907583832741,\n",
              "  0.06399010866880417,\n",
              "  0.06067777797579765,\n",
              "  0.061095889657735825,\n",
              "  0.06031009554862976,\n",
              "  0.05866360291838646,\n",
              "  0.0589921697974205,\n",
              "  0.05930034816265106,\n",
              "  0.05859009921550751,\n",
              "  0.05955193191766739,\n",
              "  0.060138802975416183,\n",
              "  0.05995793640613556,\n",
              "  0.06054602190852165,\n",
              "  0.058962948620319366,\n",
              "  0.06118335202336311,\n",
              "  0.058421388268470764,\n",
              "  0.057889364659786224,\n",
              "  0.058850616216659546,\n",
              "  0.057828448712825775,\n",
              "  0.05621128901839256,\n",
              "  0.057296670973300934,\n",
              "  0.05611681565642357,\n",
              "  0.05665496364235878,\n",
              "  0.06137640029191971,\n",
              "  0.058014996349811554],\n",
              " 'accuracy': [0.9817808866500854,\n",
              "  0.9825506806373596,\n",
              "  0.9812676310539246,\n",
              "  0.9822940826416016,\n",
              "  0.9830638766288757,\n",
              "  0.9822940826416016,\n",
              "  0.9815242290496826,\n",
              "  0.9828072786331177,\n",
              "  0.9810110330581665,\n",
              "  0.9825506806373596,\n",
              "  0.9822940826416016,\n",
              "  0.9817808866500854,\n",
              "  0.9807544350624084,\n",
              "  0.9835771322250366,\n",
              "  0.9828072786331177,\n",
              "  0.9822940826416016,\n",
              "  0.9812676310539246,\n",
              "  0.9833204746246338,\n",
              "  0.9828072786331177,\n",
              "  0.9843469262123108,\n",
              "  0.9812676310539246,\n",
              "  0.9820374846458435,\n",
              "  0.9812676310539246,\n",
              "  0.9828072786331177,\n",
              "  0.9830638766288757,\n",
              "  0.9848601222038269,\n",
              "  0.9812676310539246,\n",
              "  0.9853733777999878,\n",
              "  0.9822940826416016,\n",
              "  0.9828072786331177,\n",
              "  0.9853733777999878,\n",
              "  0.9848601222038269,\n",
              "  0.9846035242080688,\n",
              "  0.9833204746246338,\n",
              "  0.9838337302207947,\n",
              "  0.9833204746246338,\n",
              "  0.9830638766288757,\n",
              "  0.9825506806373596,\n",
              "  0.9838337302207947,\n",
              "  0.9828072786331177,\n",
              "  0.9830638766288757,\n",
              "  0.9846035242080688,\n",
              "  0.9846035242080688,\n",
              "  0.9830638766288757,\n",
              "  0.9838337302207947,\n",
              "  0.9848601222038269,\n",
              "  0.9853733777999878,\n",
              "  0.9843469262123108,\n",
              "  0.9822940826416016,\n",
              "  0.9833204746246338],\n",
              " 'val_loss': [0.06757624447345734,\n",
              "  0.0664341151714325,\n",
              "  0.06704819202423096,\n",
              "  0.06684597581624985,\n",
              "  0.06774341315031052,\n",
              "  0.06558014452457428,\n",
              "  0.0661759153008461,\n",
              "  0.06562492996454239,\n",
              "  0.06445088237524033,\n",
              "  0.06378607451915741,\n",
              "  0.06408163905143738,\n",
              "  0.07315363734960556,\n",
              "  0.06634031236171722,\n",
              "  0.06307360529899597,\n",
              "  0.06296029686927795,\n",
              "  0.06323395669460297,\n",
              "  0.06274934858083725,\n",
              "  0.06281251460313797,\n",
              "  0.061556145548820496,\n",
              "  0.063274085521698,\n",
              "  0.06160891056060791,\n",
              "  0.06104671210050583,\n",
              "  0.06181367114186287,\n",
              "  0.06223463639616966,\n",
              "  0.06189972534775734,\n",
              "  0.06199241057038307,\n",
              "  0.06116030365228653,\n",
              "  0.06236839294433594,\n",
              "  0.06081220135092735,\n",
              "  0.06037729233503342,\n",
              "  0.06044168397784233,\n",
              "  0.061609502881765366,\n",
              "  0.06075376644730568,\n",
              "  0.05897467955946922,\n",
              "  0.060222696512937546,\n",
              "  0.059435196220874786,\n",
              "  0.06355325877666473,\n",
              "  0.05828933045268059,\n",
              "  0.05965062230825424,\n",
              "  0.058910246938467026,\n",
              "  0.05922430753707886,\n",
              "  0.05782964080572128,\n",
              "  0.057189349085092545,\n",
              "  0.058134131133556366,\n",
              "  0.05762278661131859,\n",
              "  0.05798308178782463,\n",
              "  0.05754583328962326,\n",
              "  0.05759159475564957,\n",
              "  0.06391941756010056,\n",
              "  0.056653689593076706],\n",
              " 'val_accuracy': [0.9792307615280151,\n",
              "  0.9792307615280151,\n",
              "  0.9807692170143127,\n",
              "  0.9807692170143127,\n",
              "  0.9807692170143127,\n",
              "  0.9807692170143127,\n",
              "  0.9807692170143127,\n",
              "  0.9807692170143127,\n",
              "  0.9807692170143127,\n",
              "  0.9807692170143127,\n",
              "  0.9823076725006104,\n",
              "  0.9761538505554199,\n",
              "  0.9815384745597839,\n",
              "  0.9800000190734863,\n",
              "  0.9815384745597839,\n",
              "  0.9807692170143127,\n",
              "  0.9807692170143127,\n",
              "  0.9807692170143127,\n",
              "  0.9815384745597839,\n",
              "  0.9815384745597839,\n",
              "  0.9807692170143127,\n",
              "  0.9823076725006104,\n",
              "  0.9815384745597839,\n",
              "  0.9815384745597839,\n",
              "  0.9815384745597839,\n",
              "  0.9815384745597839,\n",
              "  0.9807692170143127,\n",
              "  0.9815384745597839,\n",
              "  0.9823076725006104,\n",
              "  0.9807692170143127,\n",
              "  0.9815384745597839,\n",
              "  0.9815384745597839,\n",
              "  0.9815384745597839,\n",
              "  0.9823076725006104,\n",
              "  0.9807692170143127,\n",
              "  0.9815384745597839,\n",
              "  0.9823076725006104,\n",
              "  0.9823076725006104,\n",
              "  0.9807692170143127,\n",
              "  0.9807692170143127,\n",
              "  0.9815384745597839,\n",
              "  0.9830769300460815,\n",
              "  0.9823076725006104,\n",
              "  0.9815384745597839,\n",
              "  0.9815384745597839,\n",
              "  0.9823076725006104,\n",
              "  0.9823076725006104,\n",
              "  0.9807692170143127,\n",
              "  0.9807692170143127,\n",
              "  0.9830769300460815]}"
            ]
          },
          "metadata": {},
          "execution_count": 291
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 그래프로 과적합 여부 확인\n",
        "# >> 역전파를 50번 반복하면서 과적합이 있었는가?\n",
        "# >> 적절한 학습 횟수를 정하기 위해서는 검증셋과 테스트셋의 결과를 그래프로 보는 것이 가장 좋음\n",
        "pd.DataFrame(history.history)\n",
        "# history : model.fit 함수가 반환하는 객체로, 훈련 과정에서 수집된 모든 메트릭과 손실 값을 포함\n",
        "# 속성은 딕셔너리 형태로, 키는 메트릭의 이름(예: 'loss', 'accuracy', 'val_loss', 'val_accuracy')이고,\n",
        "# 값은 각 에포크마다 해당 메트릭의 값이 리스트로 저장"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "AeR6PqVE90-c",
        "outputId": "e567d955-07d2-42d6-e516-4f7f4aedfb08"
      },
      "execution_count": 292,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        loss  accuracy  val_loss  val_accuracy\n",
              "0   0.068343  0.981781  0.067576      0.979231\n",
              "1   0.066526  0.982551  0.066434      0.979231\n",
              "2   0.065235  0.981268  0.067048      0.980769\n",
              "3   0.065815  0.982294  0.066846      0.980769\n",
              "4   0.065920  0.983064  0.067743      0.980769\n",
              "5   0.065625  0.982294  0.065580      0.980769\n",
              "6   0.064995  0.981524  0.066176      0.980769\n",
              "7   0.064404  0.982807  0.065625      0.980769\n",
              "8   0.065349  0.981011  0.064451      0.980769\n",
              "9   0.063298  0.982551  0.063786      0.980769\n",
              "10  0.064434  0.982294  0.064082      0.982308\n",
              "11  0.065037  0.981781  0.073154      0.976154\n",
              "12  0.068385  0.980754  0.066340      0.981538\n",
              "13  0.062934  0.983577  0.063074      0.980000\n",
              "14  0.063399  0.982807  0.062960      0.981538\n",
              "15  0.063195  0.982294  0.063234      0.980769\n",
              "16  0.063980  0.981268  0.062749      0.980769\n",
              "17  0.062640  0.983320  0.062813      0.980769\n",
              "18  0.063394  0.982807  0.061556      0.981538\n",
              "19  0.062181  0.984347  0.063274      0.981538\n",
              "20  0.065540  0.981268  0.061609      0.980769\n",
              "21  0.066116  0.982037  0.061047      0.982308\n",
              "22  0.064631  0.981268  0.061814      0.981538\n",
              "23  0.061307  0.982807  0.062235      0.981538\n",
              "24  0.060656  0.983064  0.061900      0.981538\n",
              "25  0.059709  0.984860  0.061992      0.981538\n",
              "26  0.063990  0.981268  0.061160      0.980769\n",
              "27  0.060678  0.985373  0.062368      0.981538\n",
              "28  0.061096  0.982294  0.060812      0.982308\n",
              "29  0.060310  0.982807  0.060377      0.980769\n",
              "30  0.058664  0.985373  0.060442      0.981538\n",
              "31  0.058992  0.984860  0.061610      0.981538\n",
              "32  0.059300  0.984604  0.060754      0.981538\n",
              "33  0.058590  0.983320  0.058975      0.982308\n",
              "34  0.059552  0.983834  0.060223      0.980769\n",
              "35  0.060139  0.983320  0.059435      0.981538\n",
              "36  0.059958  0.983064  0.063553      0.982308\n",
              "37  0.060546  0.982551  0.058289      0.982308\n",
              "38  0.058963  0.983834  0.059651      0.980769\n",
              "39  0.061183  0.982807  0.058910      0.980769\n",
              "40  0.058421  0.983064  0.059224      0.981538\n",
              "41  0.057889  0.984604  0.057830      0.983077\n",
              "42  0.058851  0.984604  0.057189      0.982308\n",
              "43  0.057828  0.983064  0.058134      0.981538\n",
              "44  0.056211  0.983834  0.057623      0.981538\n",
              "45  0.057297  0.984860  0.057983      0.982308\n",
              "46  0.056117  0.985373  0.057546      0.982308\n",
              "47  0.056655  0.984347  0.057592      0.980769\n",
              "48  0.061376  0.982294  0.063919      0.980769\n",
              "49  0.058015  0.983320  0.056654      0.983077"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-992eaffc-e342-4205-a58c-46e40705aec0\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>val_loss</th>\n",
              "      <th>val_accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.068343</td>\n",
              "      <td>0.981781</td>\n",
              "      <td>0.067576</td>\n",
              "      <td>0.979231</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.066526</td>\n",
              "      <td>0.982551</td>\n",
              "      <td>0.066434</td>\n",
              "      <td>0.979231</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.065235</td>\n",
              "      <td>0.981268</td>\n",
              "      <td>0.067048</td>\n",
              "      <td>0.980769</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.065815</td>\n",
              "      <td>0.982294</td>\n",
              "      <td>0.066846</td>\n",
              "      <td>0.980769</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.065920</td>\n",
              "      <td>0.983064</td>\n",
              "      <td>0.067743</td>\n",
              "      <td>0.980769</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.065625</td>\n",
              "      <td>0.982294</td>\n",
              "      <td>0.065580</td>\n",
              "      <td>0.980769</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.064995</td>\n",
              "      <td>0.981524</td>\n",
              "      <td>0.066176</td>\n",
              "      <td>0.980769</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.064404</td>\n",
              "      <td>0.982807</td>\n",
              "      <td>0.065625</td>\n",
              "      <td>0.980769</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.065349</td>\n",
              "      <td>0.981011</td>\n",
              "      <td>0.064451</td>\n",
              "      <td>0.980769</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.063298</td>\n",
              "      <td>0.982551</td>\n",
              "      <td>0.063786</td>\n",
              "      <td>0.980769</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.064434</td>\n",
              "      <td>0.982294</td>\n",
              "      <td>0.064082</td>\n",
              "      <td>0.982308</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.065037</td>\n",
              "      <td>0.981781</td>\n",
              "      <td>0.073154</td>\n",
              "      <td>0.976154</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.068385</td>\n",
              "      <td>0.980754</td>\n",
              "      <td>0.066340</td>\n",
              "      <td>0.981538</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.062934</td>\n",
              "      <td>0.983577</td>\n",
              "      <td>0.063074</td>\n",
              "      <td>0.980000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.063399</td>\n",
              "      <td>0.982807</td>\n",
              "      <td>0.062960</td>\n",
              "      <td>0.981538</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.063195</td>\n",
              "      <td>0.982294</td>\n",
              "      <td>0.063234</td>\n",
              "      <td>0.980769</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.063980</td>\n",
              "      <td>0.981268</td>\n",
              "      <td>0.062749</td>\n",
              "      <td>0.980769</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.062640</td>\n",
              "      <td>0.983320</td>\n",
              "      <td>0.062813</td>\n",
              "      <td>0.980769</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.063394</td>\n",
              "      <td>0.982807</td>\n",
              "      <td>0.061556</td>\n",
              "      <td>0.981538</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.062181</td>\n",
              "      <td>0.984347</td>\n",
              "      <td>0.063274</td>\n",
              "      <td>0.981538</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.065540</td>\n",
              "      <td>0.981268</td>\n",
              "      <td>0.061609</td>\n",
              "      <td>0.980769</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.066116</td>\n",
              "      <td>0.982037</td>\n",
              "      <td>0.061047</td>\n",
              "      <td>0.982308</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.064631</td>\n",
              "      <td>0.981268</td>\n",
              "      <td>0.061814</td>\n",
              "      <td>0.981538</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.061307</td>\n",
              "      <td>0.982807</td>\n",
              "      <td>0.062235</td>\n",
              "      <td>0.981538</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.060656</td>\n",
              "      <td>0.983064</td>\n",
              "      <td>0.061900</td>\n",
              "      <td>0.981538</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.059709</td>\n",
              "      <td>0.984860</td>\n",
              "      <td>0.061992</td>\n",
              "      <td>0.981538</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.063990</td>\n",
              "      <td>0.981268</td>\n",
              "      <td>0.061160</td>\n",
              "      <td>0.980769</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.060678</td>\n",
              "      <td>0.985373</td>\n",
              "      <td>0.062368</td>\n",
              "      <td>0.981538</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.061096</td>\n",
              "      <td>0.982294</td>\n",
              "      <td>0.060812</td>\n",
              "      <td>0.982308</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.060310</td>\n",
              "      <td>0.982807</td>\n",
              "      <td>0.060377</td>\n",
              "      <td>0.980769</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0.058664</td>\n",
              "      <td>0.985373</td>\n",
              "      <td>0.060442</td>\n",
              "      <td>0.981538</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>0.058992</td>\n",
              "      <td>0.984860</td>\n",
              "      <td>0.061610</td>\n",
              "      <td>0.981538</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0.059300</td>\n",
              "      <td>0.984604</td>\n",
              "      <td>0.060754</td>\n",
              "      <td>0.981538</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>0.058590</td>\n",
              "      <td>0.983320</td>\n",
              "      <td>0.058975</td>\n",
              "      <td>0.982308</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>0.059552</td>\n",
              "      <td>0.983834</td>\n",
              "      <td>0.060223</td>\n",
              "      <td>0.980769</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>0.060139</td>\n",
              "      <td>0.983320</td>\n",
              "      <td>0.059435</td>\n",
              "      <td>0.981538</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>0.059958</td>\n",
              "      <td>0.983064</td>\n",
              "      <td>0.063553</td>\n",
              "      <td>0.982308</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>0.060546</td>\n",
              "      <td>0.982551</td>\n",
              "      <td>0.058289</td>\n",
              "      <td>0.982308</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>0.058963</td>\n",
              "      <td>0.983834</td>\n",
              "      <td>0.059651</td>\n",
              "      <td>0.980769</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>0.061183</td>\n",
              "      <td>0.982807</td>\n",
              "      <td>0.058910</td>\n",
              "      <td>0.980769</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>0.058421</td>\n",
              "      <td>0.983064</td>\n",
              "      <td>0.059224</td>\n",
              "      <td>0.981538</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>0.057889</td>\n",
              "      <td>0.984604</td>\n",
              "      <td>0.057830</td>\n",
              "      <td>0.983077</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>0.058851</td>\n",
              "      <td>0.984604</td>\n",
              "      <td>0.057189</td>\n",
              "      <td>0.982308</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>0.057828</td>\n",
              "      <td>0.983064</td>\n",
              "      <td>0.058134</td>\n",
              "      <td>0.981538</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>0.056211</td>\n",
              "      <td>0.983834</td>\n",
              "      <td>0.057623</td>\n",
              "      <td>0.981538</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>0.057297</td>\n",
              "      <td>0.984860</td>\n",
              "      <td>0.057983</td>\n",
              "      <td>0.982308</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>0.056117</td>\n",
              "      <td>0.985373</td>\n",
              "      <td>0.057546</td>\n",
              "      <td>0.982308</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>0.056655</td>\n",
              "      <td>0.984347</td>\n",
              "      <td>0.057592</td>\n",
              "      <td>0.980769</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>0.061376</td>\n",
              "      <td>0.982294</td>\n",
              "      <td>0.063919</td>\n",
              "      <td>0.980769</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>0.058015</td>\n",
              "      <td>0.983320</td>\n",
              "      <td>0.056654</td>\n",
              "      <td>0.983077</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-992eaffc-e342-4205-a58c-46e40705aec0')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-992eaffc-e342-4205-a58c-46e40705aec0 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-992eaffc-e342-4205-a58c-46e40705aec0');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-da6613db-9862-4eb7-b87a-5391156f20eb\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-da6613db-9862-4eb7-b87a-5391156f20eb')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-da6613db-9862-4eb7-b87a-5391156f20eb button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"# \\uac12\\uc740 \\uac01 \\uc5d0\\ud3ec\\ud06c\\ub9c8\\ub2e4 \\ud574\\ub2f9 \\uba54\\ud2b8\\ub9ad\\uc758 \\uac12\\uc774 \\ub9ac\\uc2a4\\ud2b8\\ub85c \\uc800\\uc7a5\",\n  \"rows\": 50,\n  \"fields\": [\n    {\n      \"column\": \"loss\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0032504901053078375,\n        \"min\": 0.05611681565642357,\n        \"max\": 0.06838509440422058,\n        \"num_unique_values\": 50,\n        \"samples\": [\n          0.06293390691280365,\n          0.06118335202336311,\n          0.05866360291838646\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0012402054907710953,\n        \"min\": 0.9807544350624084,\n        \"max\": 0.9853733777999878,\n        \"num_unique_values\": 17,\n        \"samples\": [\n          0.9817808866500854,\n          0.9825506806373596,\n          0.9815242290496826\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"val_loss\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.003408522008710976,\n        \"min\": 0.056653689593076706,\n        \"max\": 0.07315363734960556,\n        \"num_unique_values\": 50,\n        \"samples\": [\n          0.06307360529899597,\n          0.058910246938467026,\n          0.06044168397784233\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"val_accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0010888551848766514,\n        \"min\": 0.9761538505554199,\n        \"max\": 0.9830769300460815,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          0.9792307615280151,\n          0.9807692170143127,\n          0.9800000190734863\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 292
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hist_df = pd.DataFrame(history.history)\n",
        "\n",
        "# val_loss, train_loss() 변화\n",
        "y_vloss = hist_df['val_loss']  # 검증 손실 값(오차)\n",
        "y_loss = hist_df['loss']  # 훈련 손실 값\n",
        "\n",
        "# x값을 지정, 검증용 셋의 오차 >> 빨간색, 학습용 셋의 오차 >> 파란색\n",
        "\n",
        "np.arange(len(y_loss))  # 훈련 손실 값 원소 개수"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zprtant-Gn2",
        "outputId": "d7f686a6-64bc-4632-ecba-a2a68ad28d8b"
      },
      "execution_count": 293,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
              "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
              "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49])"
            ]
          },
          "metadata": {},
          "execution_count": 293
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_len = np.arange(len(y_loss)) #x_len >> epochs\n",
        "\n",
        "plt.plot(x_len, y_vloss, \"o\", c='red', markersize=2, label='testset_loss')\n",
        "plt.plot(x_len, y_loss, \"o\", c='blue', markersize=2, label='trainset_loss')\n",
        "\n",
        "plt.legend(loc='best')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "SNeIORnZAYB-",
        "outputId": "2cc72f07-7ad7-4bce-b1ab-c5b906b4f3d6"
      },
      "execution_count": 294,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "early_stopping_callback 조기 중단 적용\n",
        "- 조기 중단 적용 전 정확도 95%"
      ],
      "metadata": {
        "id": "sfjENr1MZG57"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "bmVNWhBqjkja",
        "outputId": "f976a520-7874-4ad2-aa5d-cdc2eff6887d"
      },
      "execution_count": 295,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/mulcam_bigdata/09_DL/MODEL'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 295
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "modelpath = \"./CH14-4-bestmodel.hdf5\""
      ],
      "metadata": {
        "id": "k2VN00JJg9dp"
      },
      "execution_count": 296,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpointer = ModelCheckpoint(filepath=modelpath, verbose=1, save_best_only=True)\n",
        "# ModelCheckpoint는 keras에서 제공하는 콜백 함수로, 훈련 중 특정 시점에 모델을 저장\n",
        "\n",
        "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=20)\n",
        "# EarlyStopping은 훈련 중 모델의 성능이 더 이상 향상되지 않을 때 훈련을 조기에 중단하는 콜백 함수\n",
        "# monitor='val_loss': 훈련 중 검증 손실을 모니터링\n",
        "# patience=20: 개선이 없을 때 조기 중단하기 전에 기다릴 에포크 수를 지정"
      ],
      "metadata": {
        "id": "q3sGWEdkCQxh"
      },
      "execution_count": 297,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(x_train, y_train, epochs = 50, batch_size = 500,\n",
        "                    validation_split = 0.25, callbacks = [early_stopping_callback, checkpointer])\n",
        "\n",
        "# callbacks = [] : 훈련 과정 중 특정 시점에 호출되는 콜백함수 목록\n",
        "# >> early_stopping_callback : 모델의 성능이 개선되지 않을 때 훈련을 조기에 중단\n",
        "# >> checkpointer : 모델의 검증 정확도가 개선될 때마다 모델을 저장"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u971e8xLZV9B",
        "outputId": "0e3b1c73-a5e2-4373-f95e-7350f2d9594d"
      },
      "execution_count": 298,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0403 - accuracy: 0.9840\n",
            "Epoch 1: val_loss improved from inf to 0.05796, saving model to ./CH14-4-bestmodel.hdf5\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 0.0557 - accuracy: 0.9843 - val_loss: 0.0580 - val_accuracy: 0.9815\n",
            "Epoch 2/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0448 - accuracy: 0.9840\n",
            "Epoch 2: val_loss improved from 0.05796 to 0.05668, saving model to ./CH14-4-bestmodel.hdf5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8/8 [==============================] - 0s 21ms/step - loss: 0.0565 - accuracy: 0.9841 - val_loss: 0.0567 - val_accuracy: 0.9823\n",
            "Epoch 3/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0736 - accuracy: 0.9800\n",
            "Epoch 3: val_loss improved from 0.05668 to 0.05575, saving model to ./CH14-4-bestmodel.hdf5\n",
            "8/8 [==============================] - 0s 26ms/step - loss: 0.0556 - accuracy: 0.9846 - val_loss: 0.0558 - val_accuracy: 0.9831\n",
            "Epoch 4/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0350 - accuracy: 0.9840\n",
            "Epoch 4: val_loss did not improve from 0.05575\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0555 - accuracy: 0.9838 - val_loss: 0.0562 - val_accuracy: 0.9838\n",
            "Epoch 5/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0347 - accuracy: 0.9920\n",
            "Epoch 5: val_loss did not improve from 0.05575\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0561 - accuracy: 0.9831 - val_loss: 0.0562 - val_accuracy: 0.9831\n",
            "Epoch 6/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0404 - accuracy: 0.9880\n",
            "Epoch 6: val_loss did not improve from 0.05575\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0548 - accuracy: 0.9859 - val_loss: 0.0570 - val_accuracy: 0.9838\n",
            "Epoch 7/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0447 - accuracy: 0.9860\n",
            "Epoch 7: val_loss did not improve from 0.05575\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0556 - accuracy: 0.9843 - val_loss: 0.0604 - val_accuracy: 0.9800\n",
            "Epoch 8/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0454 - accuracy: 0.9860\n",
            "Epoch 8: val_loss did not improve from 0.05575\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0559 - accuracy: 0.9849 - val_loss: 0.0592 - val_accuracy: 0.9815\n",
            "Epoch 9/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0360 - accuracy: 0.9920\n",
            "Epoch 9: val_loss did not improve from 0.05575\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0602 - accuracy: 0.9833 - val_loss: 0.0697 - val_accuracy: 0.9762\n",
            "Epoch 10/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0580 - accuracy: 0.9820\n",
            "Epoch 10: val_loss did not improve from 0.05575\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0590 - accuracy: 0.9831 - val_loss: 0.0571 - val_accuracy: 0.9815\n",
            "Epoch 11/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0481 - accuracy: 0.9900\n",
            "Epoch 11: val_loss did not improve from 0.05575\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0567 - accuracy: 0.9843 - val_loss: 0.0582 - val_accuracy: 0.9815\n",
            "Epoch 12/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0623 - accuracy: 0.9780\n",
            "Epoch 12: val_loss did not improve from 0.05575\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0625 - accuracy: 0.9826 - val_loss: 0.0560 - val_accuracy: 0.9823\n",
            "Epoch 13/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0528 - accuracy: 0.9880\n",
            "Epoch 13: val_loss did not improve from 0.05575\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0580 - accuracy: 0.9831 - val_loss: 0.0571 - val_accuracy: 0.9808\n",
            "Epoch 14/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0568 - accuracy: 0.9920\n",
            "Epoch 14: val_loss did not improve from 0.05575\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0593 - accuracy: 0.9851 - val_loss: 0.0562 - val_accuracy: 0.9823\n",
            "Epoch 15/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0739 - accuracy: 0.9740\n",
            "Epoch 15: val_loss improved from 0.05575 to 0.05499, saving model to ./CH14-4-bestmodel.hdf5\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.0567 - accuracy: 0.9854 - val_loss: 0.0550 - val_accuracy: 0.9838\n",
            "Epoch 16/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0432 - accuracy: 0.9880\n",
            "Epoch 16: val_loss did not improve from 0.05499\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0555 - accuracy: 0.9854 - val_loss: 0.0558 - val_accuracy: 0.9823\n",
            "Epoch 17/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0434 - accuracy: 0.9860\n",
            "Epoch 17: val_loss did not improve from 0.05499\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0571 - accuracy: 0.9849 - val_loss: 0.0556 - val_accuracy: 0.9815\n",
            "Epoch 18/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0717 - accuracy: 0.9780\n",
            "Epoch 18: val_loss did not improve from 0.05499\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0536 - accuracy: 0.9854 - val_loss: 0.0561 - val_accuracy: 0.9838\n",
            "Epoch 19/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0562 - accuracy: 0.9820\n",
            "Epoch 19: val_loss did not improve from 0.05499\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0538 - accuracy: 0.9851 - val_loss: 0.0559 - val_accuracy: 0.9823\n",
            "Epoch 20/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0321 - accuracy: 0.9900\n",
            "Epoch 20: val_loss did not improve from 0.05499\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0541 - accuracy: 0.9846 - val_loss: 0.0552 - val_accuracy: 0.9815\n",
            "Epoch 21/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0226 - accuracy: 0.9960\n",
            "Epoch 21: val_loss did not improve from 0.05499\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0529 - accuracy: 0.9861 - val_loss: 0.0553 - val_accuracy: 0.9831\n",
            "Epoch 22/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0630 - accuracy: 0.9760\n",
            "Epoch 22: val_loss improved from 0.05499 to 0.05457, saving model to ./CH14-4-bestmodel.hdf5\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0566 - accuracy: 0.9846 - val_loss: 0.0546 - val_accuracy: 0.9823\n",
            "Epoch 23/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0785 - accuracy: 0.9760\n",
            "Epoch 23: val_loss did not improve from 0.05457\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0545 - accuracy: 0.9846 - val_loss: 0.0550 - val_accuracy: 0.9815\n",
            "Epoch 24/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0243 - accuracy: 0.9960\n",
            "Epoch 24: val_loss improved from 0.05457 to 0.05452, saving model to ./CH14-4-bestmodel.hdf5\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.0527 - accuracy: 0.9864 - val_loss: 0.0545 - val_accuracy: 0.9815\n",
            "Epoch 25/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0863 - accuracy: 0.9800\n",
            "Epoch 25: val_loss improved from 0.05452 to 0.05418, saving model to ./CH14-4-bestmodel.hdf5\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.0524 - accuracy: 0.9859 - val_loss: 0.0542 - val_accuracy: 0.9846\n",
            "Epoch 26/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0644 - accuracy: 0.9800\n",
            "Epoch 26: val_loss did not improve from 0.05418\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0531 - accuracy: 0.9851 - val_loss: 0.0569 - val_accuracy: 0.9831\n",
            "Epoch 27/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0470 - accuracy: 0.9840\n",
            "Epoch 27: val_loss improved from 0.05418 to 0.05396, saving model to ./CH14-4-bestmodel.hdf5\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.0570 - accuracy: 0.9836 - val_loss: 0.0540 - val_accuracy: 0.9846\n",
            "Epoch 28/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0324 - accuracy: 0.9900\n",
            "Epoch 28: val_loss improved from 0.05396 to 0.05364, saving model to ./CH14-4-bestmodel.hdf5\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.0533 - accuracy: 0.9869 - val_loss: 0.0536 - val_accuracy: 0.9854\n",
            "Epoch 29/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0390 - accuracy: 0.9880\n",
            "Epoch 29: val_loss did not improve from 0.05364\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0540 - accuracy: 0.9856 - val_loss: 0.0545 - val_accuracy: 0.9808\n",
            "Epoch 30/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0500 - accuracy: 0.9860\n",
            "Epoch 30: val_loss did not improve from 0.05364\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0526 - accuracy: 0.9859 - val_loss: 0.0548 - val_accuracy: 0.9831\n",
            "Epoch 31/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0475 - accuracy: 0.9840\n",
            "Epoch 31: val_loss did not improve from 0.05364\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0541 - accuracy: 0.9861 - val_loss: 0.0601 - val_accuracy: 0.9823\n",
            "Epoch 32/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0751 - accuracy: 0.9780\n",
            "Epoch 32: val_loss did not improve from 0.05364\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0564 - accuracy: 0.9846 - val_loss: 0.0596 - val_accuracy: 0.9823\n",
            "Epoch 33/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0366 - accuracy: 0.9860\n",
            "Epoch 33: val_loss did not improve from 0.05364\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0559 - accuracy: 0.9851 - val_loss: 0.0607 - val_accuracy: 0.9823\n",
            "Epoch 34/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0589 - accuracy: 0.9860\n",
            "Epoch 34: val_loss did not improve from 0.05364\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0541 - accuracy: 0.9846 - val_loss: 0.0542 - val_accuracy: 0.9823\n",
            "Epoch 35/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0433 - accuracy: 0.9820\n",
            "Epoch 35: val_loss did not improve from 0.05364\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0523 - accuracy: 0.9867 - val_loss: 0.0540 - val_accuracy: 0.9831\n",
            "Epoch 36/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0813 - accuracy: 0.9760\n",
            "Epoch 36: val_loss did not improve from 0.05364\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0524 - accuracy: 0.9861 - val_loss: 0.0559 - val_accuracy: 0.9831\n",
            "Epoch 37/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0692 - accuracy: 0.9800\n",
            "Epoch 37: val_loss did not improve from 0.05364\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0518 - accuracy: 0.9846 - val_loss: 0.0544 - val_accuracy: 0.9831\n",
            "Epoch 38/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0548 - accuracy: 0.9840\n",
            "Epoch 38: val_loss did not improve from 0.05364\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0514 - accuracy: 0.9849 - val_loss: 0.0561 - val_accuracy: 0.9831\n",
            "Epoch 39/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0521 - accuracy: 0.9860\n",
            "Epoch 39: val_loss did not improve from 0.05364\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0536 - accuracy: 0.9851 - val_loss: 0.0551 - val_accuracy: 0.9831\n",
            "Epoch 40/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0398 - accuracy: 0.9860\n",
            "Epoch 40: val_loss improved from 0.05364 to 0.05311, saving model to ./CH14-4-bestmodel.hdf5\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.0518 - accuracy: 0.9859 - val_loss: 0.0531 - val_accuracy: 0.9831\n",
            "Epoch 41/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0497 - accuracy: 0.9900\n",
            "Epoch 41: val_loss improved from 0.05311 to 0.05293, saving model to ./CH14-4-bestmodel.hdf5\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0504 - accuracy: 0.9856 - val_loss: 0.0529 - val_accuracy: 0.9854\n",
            "Epoch 42/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0668 - accuracy: 0.9840\n",
            "Epoch 42: val_loss did not improve from 0.05293\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0511 - accuracy: 0.9869 - val_loss: 0.0568 - val_accuracy: 0.9838\n",
            "Epoch 43/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0653 - accuracy: 0.9780\n",
            "Epoch 43: val_loss improved from 0.05293 to 0.05293, saving model to ./CH14-4-bestmodel.hdf5\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.0535 - accuracy: 0.9854 - val_loss: 0.0529 - val_accuracy: 0.9846\n",
            "Epoch 44/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0638 - accuracy: 0.9840\n",
            "Epoch 44: val_loss improved from 0.05293 to 0.05215, saving model to ./CH14-4-bestmodel.hdf5\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.0506 - accuracy: 0.9864 - val_loss: 0.0521 - val_accuracy: 0.9831\n",
            "Epoch 45/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0439 - accuracy: 0.9880\n",
            "Epoch 45: val_loss did not improve from 0.05215\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0510 - accuracy: 0.9867 - val_loss: 0.0574 - val_accuracy: 0.9831\n",
            "Epoch 46/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0843 - accuracy: 0.9740\n",
            "Epoch 46: val_loss improved from 0.05215 to 0.05169, saving model to ./CH14-4-bestmodel.hdf5\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.0529 - accuracy: 0.9869 - val_loss: 0.0517 - val_accuracy: 0.9846\n",
            "Epoch 47/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0526 - accuracy: 0.9840\n",
            "Epoch 47: val_loss did not improve from 0.05169\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0508 - accuracy: 0.9864 - val_loss: 0.0531 - val_accuracy: 0.9862\n",
            "Epoch 48/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0421 - accuracy: 0.9880\n",
            "Epoch 48: val_loss did not improve from 0.05169\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0498 - accuracy: 0.9869 - val_loss: 0.0529 - val_accuracy: 0.9831\n",
            "Epoch 49/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0328 - accuracy: 0.9940\n",
            "Epoch 49: val_loss did not improve from 0.05169\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0505 - accuracy: 0.9851 - val_loss: 0.0545 - val_accuracy: 0.9831\n",
            "Epoch 50/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0520 - accuracy: 0.9820\n",
            "Epoch 50: val_loss did not improve from 0.05169\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0514 - accuracy: 0.9867 - val_loss: 0.0604 - val_accuracy: 0.9831\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 테스트 결과 출력\n",
        "score = model.evaluate(x_test, y_test)\n",
        "print('Test Dataset ACC: ', round(score[1],2))\n",
        "# Test Dataset ACC: 0.98"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wniqPiOwZg3x",
        "outputId": "2868bcad-3ab7-4f90-812c-5aeecb99dd6d"
      },
      "execution_count": 299,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "41/41 [==============================] - 0s 2ms/step - loss: 0.0719 - accuracy: 0.9769\n",
            "Test Dataset ACC:  0.98\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 그래프로 과적합 여부 확인\n",
        "\n",
        "pd.DataFrame(history.history)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FZV5rYyibJmD",
        "outputId": "01ab29ec-df2e-422e-b5e5-1ac4e03eea66"
      },
      "execution_count": 300,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        loss  accuracy  val_loss  val_accuracy\n",
              "0   0.055673  0.984347  0.057956      0.981538\n",
              "1   0.056511  0.984090  0.056678      0.982308\n",
              "2   0.055621  0.984604  0.055751      0.983077\n",
              "3   0.055459  0.983834  0.056169      0.983846\n",
              "4   0.056050  0.983064  0.056159      0.983077\n",
              "5   0.054848  0.985887  0.056952      0.983846\n",
              "6   0.055647  0.984347  0.060371      0.980000\n",
              "7   0.055874  0.984860  0.059178      0.981538\n",
              "8   0.060206  0.983320  0.069748      0.976154\n",
              "9   0.058985  0.983064  0.057058      0.981538\n",
              "10  0.056661  0.984347  0.058178      0.981538\n",
              "11  0.062523  0.982551  0.056048      0.982308\n",
              "12  0.058001  0.983064  0.057102      0.980769\n",
              "13  0.059324  0.985117  0.056200      0.982308\n",
              "14  0.056705  0.985373  0.054985      0.983846\n",
              "15  0.055490  0.985373  0.055839      0.982308\n",
              "16  0.057120  0.984860  0.055621      0.981538\n",
              "17  0.053580  0.985373  0.056132      0.983846\n",
              "18  0.053788  0.985117  0.055879      0.982308\n",
              "19  0.054107  0.984604  0.055221      0.981538\n",
              "20  0.052933  0.986143  0.055314      0.983077\n",
              "21  0.056556  0.984604  0.054574      0.982308\n",
              "22  0.054548  0.984604  0.055046      0.981538\n",
              "23  0.052713  0.986400  0.054523      0.981538\n",
              "24  0.052415  0.985887  0.054176      0.984615\n",
              "25  0.053146  0.985117  0.056893      0.983077\n",
              "26  0.056955  0.983577  0.053961      0.984615\n",
              "27  0.053327  0.986913  0.053643      0.985385\n",
              "28  0.054049  0.985630  0.054475      0.980769\n",
              "29  0.052576  0.985887  0.054831      0.983077\n",
              "30  0.054089  0.986143  0.060091      0.982308\n",
              "31  0.056432  0.984604  0.059645      0.982308\n",
              "32  0.055933  0.985117  0.060738      0.982308\n",
              "33  0.054081  0.984604  0.054195      0.982308\n",
              "34  0.052301  0.986656  0.053984      0.983077\n",
              "35  0.052391  0.986143  0.055943      0.983077\n",
              "36  0.051785  0.984604  0.054429      0.983077\n",
              "37  0.051351  0.984860  0.056093      0.983077\n",
              "38  0.053622  0.985117  0.055145      0.983077\n",
              "39  0.051799  0.985887  0.053109      0.983077\n",
              "40  0.050424  0.985630  0.052932      0.985385\n",
              "41  0.051105  0.986913  0.056845      0.983846\n",
              "42  0.053530  0.985373  0.052929      0.984615\n",
              "43  0.050572  0.986400  0.052150      0.983077\n",
              "44  0.050974  0.986656  0.057396      0.983077\n",
              "45  0.052934  0.986913  0.051694      0.984615\n",
              "46  0.050761  0.986400  0.053115      0.986154\n",
              "47  0.049777  0.986913  0.052947      0.983077\n",
              "48  0.050485  0.985117  0.054546      0.983077\n",
              "49  0.051403  0.986656  0.060382      0.983077"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2e9c18d0-fcfc-437e-87e6-5c1130b4e3a8\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>val_loss</th>\n",
              "      <th>val_accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.055673</td>\n",
              "      <td>0.984347</td>\n",
              "      <td>0.057956</td>\n",
              "      <td>0.981538</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.056511</td>\n",
              "      <td>0.984090</td>\n",
              "      <td>0.056678</td>\n",
              "      <td>0.982308</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.055621</td>\n",
              "      <td>0.984604</td>\n",
              "      <td>0.055751</td>\n",
              "      <td>0.983077</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.055459</td>\n",
              "      <td>0.983834</td>\n",
              "      <td>0.056169</td>\n",
              "      <td>0.983846</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.056050</td>\n",
              "      <td>0.983064</td>\n",
              "      <td>0.056159</td>\n",
              "      <td>0.983077</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.054848</td>\n",
              "      <td>0.985887</td>\n",
              "      <td>0.056952</td>\n",
              "      <td>0.983846</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.055647</td>\n",
              "      <td>0.984347</td>\n",
              "      <td>0.060371</td>\n",
              "      <td>0.980000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.055874</td>\n",
              "      <td>0.984860</td>\n",
              "      <td>0.059178</td>\n",
              "      <td>0.981538</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.060206</td>\n",
              "      <td>0.983320</td>\n",
              "      <td>0.069748</td>\n",
              "      <td>0.976154</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.058985</td>\n",
              "      <td>0.983064</td>\n",
              "      <td>0.057058</td>\n",
              "      <td>0.981538</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.056661</td>\n",
              "      <td>0.984347</td>\n",
              "      <td>0.058178</td>\n",
              "      <td>0.981538</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.062523</td>\n",
              "      <td>0.982551</td>\n",
              "      <td>0.056048</td>\n",
              "      <td>0.982308</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.058001</td>\n",
              "      <td>0.983064</td>\n",
              "      <td>0.057102</td>\n",
              "      <td>0.980769</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.059324</td>\n",
              "      <td>0.985117</td>\n",
              "      <td>0.056200</td>\n",
              "      <td>0.982308</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.056705</td>\n",
              "      <td>0.985373</td>\n",
              "      <td>0.054985</td>\n",
              "      <td>0.983846</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.055490</td>\n",
              "      <td>0.985373</td>\n",
              "      <td>0.055839</td>\n",
              "      <td>0.982308</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.057120</td>\n",
              "      <td>0.984860</td>\n",
              "      <td>0.055621</td>\n",
              "      <td>0.981538</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.053580</td>\n",
              "      <td>0.985373</td>\n",
              "      <td>0.056132</td>\n",
              "      <td>0.983846</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.053788</td>\n",
              "      <td>0.985117</td>\n",
              "      <td>0.055879</td>\n",
              "      <td>0.982308</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.054107</td>\n",
              "      <td>0.984604</td>\n",
              "      <td>0.055221</td>\n",
              "      <td>0.981538</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.052933</td>\n",
              "      <td>0.986143</td>\n",
              "      <td>0.055314</td>\n",
              "      <td>0.983077</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.056556</td>\n",
              "      <td>0.984604</td>\n",
              "      <td>0.054574</td>\n",
              "      <td>0.982308</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.054548</td>\n",
              "      <td>0.984604</td>\n",
              "      <td>0.055046</td>\n",
              "      <td>0.981538</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.052713</td>\n",
              "      <td>0.986400</td>\n",
              "      <td>0.054523</td>\n",
              "      <td>0.981538</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.052415</td>\n",
              "      <td>0.985887</td>\n",
              "      <td>0.054176</td>\n",
              "      <td>0.984615</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.053146</td>\n",
              "      <td>0.985117</td>\n",
              "      <td>0.056893</td>\n",
              "      <td>0.983077</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.056955</td>\n",
              "      <td>0.983577</td>\n",
              "      <td>0.053961</td>\n",
              "      <td>0.984615</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.053327</td>\n",
              "      <td>0.986913</td>\n",
              "      <td>0.053643</td>\n",
              "      <td>0.985385</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.054049</td>\n",
              "      <td>0.985630</td>\n",
              "      <td>0.054475</td>\n",
              "      <td>0.980769</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.052576</td>\n",
              "      <td>0.985887</td>\n",
              "      <td>0.054831</td>\n",
              "      <td>0.983077</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0.054089</td>\n",
              "      <td>0.986143</td>\n",
              "      <td>0.060091</td>\n",
              "      <td>0.982308</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>0.056432</td>\n",
              "      <td>0.984604</td>\n",
              "      <td>0.059645</td>\n",
              "      <td>0.982308</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0.055933</td>\n",
              "      <td>0.985117</td>\n",
              "      <td>0.060738</td>\n",
              "      <td>0.982308</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>0.054081</td>\n",
              "      <td>0.984604</td>\n",
              "      <td>0.054195</td>\n",
              "      <td>0.982308</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>0.052301</td>\n",
              "      <td>0.986656</td>\n",
              "      <td>0.053984</td>\n",
              "      <td>0.983077</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>0.052391</td>\n",
              "      <td>0.986143</td>\n",
              "      <td>0.055943</td>\n",
              "      <td>0.983077</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>0.051785</td>\n",
              "      <td>0.984604</td>\n",
              "      <td>0.054429</td>\n",
              "      <td>0.983077</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>0.051351</td>\n",
              "      <td>0.984860</td>\n",
              "      <td>0.056093</td>\n",
              "      <td>0.983077</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>0.053622</td>\n",
              "      <td>0.985117</td>\n",
              "      <td>0.055145</td>\n",
              "      <td>0.983077</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>0.051799</td>\n",
              "      <td>0.985887</td>\n",
              "      <td>0.053109</td>\n",
              "      <td>0.983077</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>0.050424</td>\n",
              "      <td>0.985630</td>\n",
              "      <td>0.052932</td>\n",
              "      <td>0.985385</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>0.051105</td>\n",
              "      <td>0.986913</td>\n",
              "      <td>0.056845</td>\n",
              "      <td>0.983846</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>0.053530</td>\n",
              "      <td>0.985373</td>\n",
              "      <td>0.052929</td>\n",
              "      <td>0.984615</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>0.050572</td>\n",
              "      <td>0.986400</td>\n",
              "      <td>0.052150</td>\n",
              "      <td>0.983077</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>0.050974</td>\n",
              "      <td>0.986656</td>\n",
              "      <td>0.057396</td>\n",
              "      <td>0.983077</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>0.052934</td>\n",
              "      <td>0.986913</td>\n",
              "      <td>0.051694</td>\n",
              "      <td>0.984615</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>0.050761</td>\n",
              "      <td>0.986400</td>\n",
              "      <td>0.053115</td>\n",
              "      <td>0.986154</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>0.049777</td>\n",
              "      <td>0.986913</td>\n",
              "      <td>0.052947</td>\n",
              "      <td>0.983077</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>0.050485</td>\n",
              "      <td>0.985117</td>\n",
              "      <td>0.054546</td>\n",
              "      <td>0.983077</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>0.051403</td>\n",
              "      <td>0.986656</td>\n",
              "      <td>0.060382</td>\n",
              "      <td>0.983077</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2e9c18d0-fcfc-437e-87e6-5c1130b4e3a8')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-2e9c18d0-fcfc-437e-87e6-5c1130b4e3a8 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-2e9c18d0-fcfc-437e-87e6-5c1130b4e3a8');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-a461ba38-0632-4e6a-a346-bb616bee512f\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a461ba38-0632-4e6a-a346-bb616bee512f')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-a461ba38-0632-4e6a-a346-bb616bee512f button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"pd\",\n  \"rows\": 50,\n  \"fields\": [\n    {\n      \"column\": \"loss\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.002759226846845435,\n        \"min\": 0.04977743700146675,\n        \"max\": 0.0625230148434639,\n        \"num_unique_values\": 50,\n        \"samples\": [\n          0.0593235157430172,\n          0.05179936811327934,\n          0.054088518023490906\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0011290433549256843,\n        \"min\": 0.9825506806373596,\n        \"max\": 0.9869130253791809,\n        \"num_unique_values\": 17,\n        \"samples\": [\n          0.9843469262123108,\n          0.9840903282165527,\n          0.9858865737915039\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"val_loss\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0029298682077964692,\n        \"min\": 0.051694389432668686,\n        \"max\": 0.0697476863861084,\n        \"num_unique_values\": 50,\n        \"samples\": [\n          0.05619962885975838,\n          0.05310855433344841,\n          0.060091204941272736\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"val_accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0015630680436890972,\n        \"min\": 0.9761538505554199,\n        \"max\": 0.9861538410186768,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.9853846430778503,\n          0.9823076725006104,\n          0.9761538505554199\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 300
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(x_train, y_train, epochs = 50, batch_size = 500,\n",
        "                    validation_split = 0.25, callbacks = [early_stopping_callback, checkpointer])\n",
        "\n",
        "hist_df = pd.DataFrame(history.history)\n",
        "\n",
        "# val_loss, val_acc 변화 보고 싶어요\n",
        "\n",
        "y_vloss = hist_df['val_loss']\n",
        "y_loss = hist_df['loss']\n",
        "\n",
        "# x값을 지정,검증용 셋의 오차 >> 빨간색, 학습용 셋의 오차 >> 파란색\n",
        "\n",
        "np.arange(len(y_loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4khiRQNubLIQ",
        "outputId": "ea607dbe-e7bc-4326-a971-53f95008a0eb"
      },
      "execution_count": 301,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0869 - accuracy: 0.9760\n",
            "Epoch 1: val_loss did not improve from 0.05169\n",
            "8/8 [==============================] - 0s 28ms/step - loss: 0.0568 - accuracy: 0.9849 - val_loss: 0.0626 - val_accuracy: 0.9831\n",
            "Epoch 2/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0677 - accuracy: 0.9860\n",
            "Epoch 2: val_loss did not improve from 0.05169\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0528 - accuracy: 0.9859 - val_loss: 0.0749 - val_accuracy: 0.9738\n",
            "Epoch 3/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0943 - accuracy: 0.9760\n",
            "Epoch 3: val_loss did not improve from 0.05169\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0590 - accuracy: 0.9833 - val_loss: 0.0555 - val_accuracy: 0.9815\n",
            "Epoch 4/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0594 - accuracy: 0.9880\n",
            "Epoch 4: val_loss did not improve from 0.05169\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0504 - accuracy: 0.9867 - val_loss: 0.0519 - val_accuracy: 0.9846\n",
            "Epoch 5/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0495 - accuracy: 0.9880\n",
            "Epoch 5: val_loss did not improve from 0.05169\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0493 - accuracy: 0.9872 - val_loss: 0.0523 - val_accuracy: 0.9831\n",
            "Epoch 6/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0942 - accuracy: 0.9780\n",
            "Epoch 6: val_loss did not improve from 0.05169\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0496 - accuracy: 0.9864 - val_loss: 0.0518 - val_accuracy: 0.9838\n",
            "Epoch 7/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0351 - accuracy: 0.9920\n",
            "Epoch 7: val_loss did not improve from 0.05169\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0485 - accuracy: 0.9877 - val_loss: 0.0537 - val_accuracy: 0.9831\n",
            "Epoch 8/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0350 - accuracy: 0.9840\n",
            "Epoch 8: val_loss did not improve from 0.05169\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0511 - accuracy: 0.9854 - val_loss: 0.0546 - val_accuracy: 0.9831\n",
            "Epoch 9/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0402 - accuracy: 0.9880\n",
            "Epoch 9: val_loss did not improve from 0.05169\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0517 - accuracy: 0.9846 - val_loss: 0.0519 - val_accuracy: 0.9838\n",
            "Epoch 10/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0636 - accuracy: 0.9820\n",
            "Epoch 10: val_loss did not improve from 0.05169\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0499 - accuracy: 0.9867 - val_loss: 0.0524 - val_accuracy: 0.9846\n",
            "Epoch 11/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0574 - accuracy: 0.9780\n",
            "Epoch 11: val_loss did not improve from 0.05169\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0489 - accuracy: 0.9872 - val_loss: 0.0546 - val_accuracy: 0.9831\n",
            "Epoch 12/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0582 - accuracy: 0.9800\n",
            "Epoch 12: val_loss improved from 0.05169 to 0.05142, saving model to ./CH14-4-bestmodel.hdf5\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.0519 - accuracy: 0.9867 - val_loss: 0.0514 - val_accuracy: 0.9838\n",
            "Epoch 13/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0454 - accuracy: 0.9860\n",
            "Epoch 13: val_loss improved from 0.05142 to 0.05133, saving model to ./CH14-4-bestmodel.hdf5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8/8 [==============================] - 0s 27ms/step - loss: 0.0489 - accuracy: 0.9861 - val_loss: 0.0513 - val_accuracy: 0.9846\n",
            "Epoch 14/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0294 - accuracy: 0.9900\n",
            "Epoch 14: val_loss improved from 0.05133 to 0.05070, saving model to ./CH14-4-bestmodel.hdf5\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.0488 - accuracy: 0.9874 - val_loss: 0.0507 - val_accuracy: 0.9846\n",
            "Epoch 15/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0244 - accuracy: 0.9940\n",
            "Epoch 15: val_loss did not improve from 0.05070\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0488 - accuracy: 0.9869 - val_loss: 0.0521 - val_accuracy: 0.9862\n",
            "Epoch 16/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0219 - accuracy: 0.9940\n",
            "Epoch 16: val_loss did not improve from 0.05070\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0484 - accuracy: 0.9879 - val_loss: 0.0563 - val_accuracy: 0.9831\n",
            "Epoch 17/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0663 - accuracy: 0.9840\n",
            "Epoch 17: val_loss did not improve from 0.05070\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0505 - accuracy: 0.9869 - val_loss: 0.0509 - val_accuracy: 0.9838\n",
            "Epoch 18/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0596 - accuracy: 0.9840\n",
            "Epoch 18: val_loss did not improve from 0.05070\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0498 - accuracy: 0.9859 - val_loss: 0.0511 - val_accuracy: 0.9854\n",
            "Epoch 19/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0631 - accuracy: 0.9820\n",
            "Epoch 19: val_loss did not improve from 0.05070\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0480 - accuracy: 0.9874 - val_loss: 0.0512 - val_accuracy: 0.9838\n",
            "Epoch 20/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0271 - accuracy: 0.9940\n",
            "Epoch 20: val_loss did not improve from 0.05070\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0486 - accuracy: 0.9861 - val_loss: 0.0508 - val_accuracy: 0.9838\n",
            "Epoch 21/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0714 - accuracy: 0.9820\n",
            "Epoch 21: val_loss did not improve from 0.05070\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0474 - accuracy: 0.9874 - val_loss: 0.0515 - val_accuracy: 0.9846\n",
            "Epoch 22/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0455 - accuracy: 0.9860\n",
            "Epoch 22: val_loss improved from 0.05070 to 0.04999, saving model to ./CH14-4-bestmodel.hdf5\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.0481 - accuracy: 0.9877 - val_loss: 0.0500 - val_accuracy: 0.9854\n",
            "Epoch 23/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0328 - accuracy: 0.9900\n",
            "Epoch 23: val_loss did not improve from 0.04999\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0487 - accuracy: 0.9877 - val_loss: 0.0506 - val_accuracy: 0.9838\n",
            "Epoch 24/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0492 - accuracy: 0.9880\n",
            "Epoch 24: val_loss did not improve from 0.04999\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0521 - accuracy: 0.9856 - val_loss: 0.0729 - val_accuracy: 0.9808\n",
            "Epoch 25/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0471 - accuracy: 0.9860\n",
            "Epoch 25: val_loss did not improve from 0.04999\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0608 - accuracy: 0.9823 - val_loss: 0.0665 - val_accuracy: 0.9808\n",
            "Epoch 26/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0987 - accuracy: 0.9700\n",
            "Epoch 26: val_loss did not improve from 0.04999\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0597 - accuracy: 0.9828 - val_loss: 0.0543 - val_accuracy: 0.9838\n",
            "Epoch 27/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0463 - accuracy: 0.9880\n",
            "Epoch 27: val_loss did not improve from 0.04999\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0534 - accuracy: 0.9854 - val_loss: 0.0507 - val_accuracy: 0.9854\n",
            "Epoch 28/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0389 - accuracy: 0.9880\n",
            "Epoch 28: val_loss did not improve from 0.04999\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0494 - accuracy: 0.9872 - val_loss: 0.0548 - val_accuracy: 0.9846\n",
            "Epoch 29/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0492 - accuracy: 0.9860\n",
            "Epoch 29: val_loss did not improve from 0.04999\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0525 - accuracy: 0.9843 - val_loss: 0.0518 - val_accuracy: 0.9846\n",
            "Epoch 30/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0499 - accuracy: 0.9880\n",
            "Epoch 30: val_loss did not improve from 0.04999\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0479 - accuracy: 0.9877 - val_loss: 0.0528 - val_accuracy: 0.9838\n",
            "Epoch 31/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0563 - accuracy: 0.9900\n",
            "Epoch 31: val_loss did not improve from 0.04999\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0471 - accuracy: 0.9885 - val_loss: 0.0520 - val_accuracy: 0.9854\n",
            "Epoch 32/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0230 - accuracy: 0.9900\n",
            "Epoch 32: val_loss did not improve from 0.04999\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0525 - accuracy: 0.9849 - val_loss: 0.0562 - val_accuracy: 0.9831\n",
            "Epoch 33/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0616 - accuracy: 0.9800\n",
            "Epoch 33: val_loss did not improve from 0.04999\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.0482 - accuracy: 0.9864 - val_loss: 0.0503 - val_accuracy: 0.9846\n",
            "Epoch 34/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0303 - accuracy: 0.9880\n",
            "Epoch 34: val_loss improved from 0.04999 to 0.04953, saving model to ./CH14-4-bestmodel.hdf5\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.0473 - accuracy: 0.9882 - val_loss: 0.0495 - val_accuracy: 0.9846\n",
            "Epoch 35/50\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.0484 - accuracy: 0.9891\n",
            "Epoch 35: val_loss did not improve from 0.04953\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.0467 - accuracy: 0.9895 - val_loss: 0.0547 - val_accuracy: 0.9831\n",
            "Epoch 36/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0649 - accuracy: 0.9820\n",
            "Epoch 36: val_loss did not improve from 0.04953\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.0493 - accuracy: 0.9856 - val_loss: 0.0596 - val_accuracy: 0.9831\n",
            "Epoch 37/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0526 - accuracy: 0.9860\n",
            "Epoch 37: val_loss did not improve from 0.04953\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0532 - accuracy: 0.9869 - val_loss: 0.0613 - val_accuracy: 0.9815\n",
            "Epoch 38/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0381 - accuracy: 0.9860\n",
            "Epoch 38: val_loss did not improve from 0.04953\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0510 - accuracy: 0.9861 - val_loss: 0.0538 - val_accuracy: 0.9846\n",
            "Epoch 39/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0565 - accuracy: 0.9760\n",
            "Epoch 39: val_loss did not improve from 0.04953\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0521 - accuracy: 0.9859 - val_loss: 0.0505 - val_accuracy: 0.9854\n",
            "Epoch 40/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0521 - accuracy: 0.9900\n",
            "Epoch 40: val_loss did not improve from 0.04953\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0496 - accuracy: 0.9874 - val_loss: 0.0506 - val_accuracy: 0.9854\n",
            "Epoch 41/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0676 - accuracy: 0.9840\n",
            "Epoch 41: val_loss did not improve from 0.04953\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0467 - accuracy: 0.9885 - val_loss: 0.0498 - val_accuracy: 0.9854\n",
            "Epoch 42/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0751 - accuracy: 0.9800\n",
            "Epoch 42: val_loss did not improve from 0.04953\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0459 - accuracy: 0.9890 - val_loss: 0.0510 - val_accuracy: 0.9838\n",
            "Epoch 43/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0250 - accuracy: 0.9940\n",
            "Epoch 43: val_loss did not improve from 0.04953\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0458 - accuracy: 0.9895 - val_loss: 0.0496 - val_accuracy: 0.9862\n",
            "Epoch 44/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0518 - accuracy: 0.9900\n",
            "Epoch 44: val_loss did not improve from 0.04953\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0466 - accuracy: 0.9885 - val_loss: 0.0498 - val_accuracy: 0.9854\n",
            "Epoch 45/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0404 - accuracy: 0.9880\n",
            "Epoch 45: val_loss improved from 0.04953 to 0.04946, saving model to ./CH14-4-bestmodel.hdf5\n",
            "8/8 [==============================] - 0s 27ms/step - loss: 0.0456 - accuracy: 0.9885 - val_loss: 0.0495 - val_accuracy: 0.9862\n",
            "Epoch 46/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0245 - accuracy: 0.9940\n",
            "Epoch 46: val_loss did not improve from 0.04946\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0463 - accuracy: 0.9877 - val_loss: 0.0495 - val_accuracy: 0.9862\n",
            "Epoch 47/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0274 - accuracy: 0.9900\n",
            "Epoch 47: val_loss did not improve from 0.04946\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0479 - accuracy: 0.9861 - val_loss: 0.0500 - val_accuracy: 0.9862\n",
            "Epoch 48/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0196 - accuracy: 0.9960\n",
            "Epoch 48: val_loss did not improve from 0.04946\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0507 - accuracy: 0.9856 - val_loss: 0.0605 - val_accuracy: 0.9823\n",
            "Epoch 49/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0612 - accuracy: 0.9820\n",
            "Epoch 49: val_loss did not improve from 0.04946\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0561 - accuracy: 0.9838 - val_loss: 0.0574 - val_accuracy: 0.9838\n",
            "Epoch 50/50\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0895 - accuracy: 0.9780\n",
            "Epoch 50: val_loss did not improve from 0.04946\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0504 - accuracy: 0.9869 - val_loss: 0.0523 - val_accuracy: 0.9831\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
              "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
              "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49])"
            ]
          },
          "metadata": {},
          "execution_count": 301
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_len = np.arange(len(y_loss)) #x_len >> epochs\n",
        "\n",
        "plt.plot(x_len, y_vloss, \"o\", c='red', markersize=2, label='val_loss')\n",
        "plt.plot(x_len, y_loss, \"o\", c='blue', markersize=2, label='loss')\n",
        "\n",
        "plt.legend(loc='best')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "1sjGUxGxbMWr",
        "outputId": "c8f12788-f341-472e-83dd-d9fe0ba7a467"
      },
      "execution_count": 302,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gwEPs6gmhNWS"
      },
      "execution_count": 302,
      "outputs": []
    }
  ]
}